Color Graphics Adapter.
The Color Graphics Adapter (CGA), introduced in 1981, was IBM's first color graphics card (originally sold under the name "Color/Graphics Monitor Adapter"), and the first color computer display standard for the IBM PC.
The standard IBM CGA graphics card is equipped with 16 kilobytes of video memory, and can be connected either to a NTSC-compatible monitor or TV via an RCA jack, or to a dedicated RGBI CRT monitor. Based around the Motorola MC6845 display controller, the CGA card features several graphics and text modes. The highest resolution of any mode is 640×200, and the highest color depth supported is 4-bit (16 colors).
The CGA color palette.
The CGA's maximum color depth of four bits results in a palette of 16 colors. The lower three bits, representing red, green and blue, corresponded to the three cathode rays, with black meaning all three cathodes are almost off. Cyan is a mixture of blue and green, magenta was blue and red, and orange-brown is green and red. White (or light gray) uses all three cathodes.
The remaining 8 colors are achieved by turning on a fourth "intensifier" bit, giving a brighter version of each color, although the dark gray color is indistinguishable from black with many monitors. CGA's "RGB plus intensity bit" design is also called "RGBI".
Standard text modes.
In every text mode, each character has a background and a foreground color — e.g. red on yellow text for one character, white on black for the next, etc. While the same 4-bit nybble used for the foreground color would normally allow all 16 colors to be used for the background color, the most significant bit of the background nybble is also used to denote whether or not the character should blink (a hardware effect offered by CGA independent of the CPU). The blinking attribute effect is enabled by default, so disabling it is the only way to freely choose the latter 8 color indexes (8-15) for the background color.
Standard RGB graphics modes.
By setting the high-intensity bit, brighter versions of these modes can be accessed.
In text mode, font bitmap data comes from the character ROM on the card, which is only available to the card itself. In graphics modes, text output by the BIOS uses two separate tables: The first half of the character set (128 characters) is supplied by a table in the BIOS ROM chip on the computer's mainboard at F000:FA6E, and the second half is supplied by the location pointed to by interrupt 1F (0000:007C). The second half of the character set will display as blanks (or garbage, depending on implementation) unless they are explicitly defined, usually by a utility such as GRAFTABL or by the calling program.
Further RGB graphics modes and tweaks.
A number of official and unofficial features exist that can be exploited to achieve better graphics on an RGBI monitor.
Some of these above tweaks can even be combined. Examples can be found in several games. Most software titles did not use these possibilities, but there were a few impressive exceptions.
The 160×100 16 color mode.
Technically, this mode is not a graphics mode, but a tweak of the 80×25 text mode. The character cell height register is changed to display only 2 lines per character cell instead of the normal 8 lines. This quadruples the number of text rows displayed from 25 to 100. These "tightly squeezed" text characters are not full characters. The system only displayes their top two lines of pixels (8 each) before moving on to the next row.
Character 221 in the extended ASCII character set consists of a box occupying the entire left half of the character matrix. (Character 222 consists of a box occupying the entire right half.)
Because each character can be assigned different foreground and background colors, it can be colored (for example) blue on the left (foreground color) and bright red on the right (background color). This can be reversed by swapping the foreground and background colors.
Using either character 221 or 222, each half of each truncated character cell can thus be treated as an individual pixel — making 160 horizontal pixels available per line. Thus, 160×100 pixels at 16 colors, with an aspect ratio of 1:1.2, are possible.
Although a roundabout way of achieving 16 color graphics display, this works quite well and the mode is even mentioned (although not explained) in IBM's official hardware documentation.
More detail can be achieved in this mode by using other characters, combining ASCII art with the aforesaid technique.
Because the CGA has 16384 bytes of graphics memory, not 16000, it is just as easy to set the number of lines in this mode to 102 instead of 100 for a resolution of 160×102. This uses extra video memory that is normally unused. However, most games did not do this, perhaps out of fear it would only work on some monitors but not others.
The same text cell height reduction technique can also be used with the 40×25 text mode. This only made sense when using ASCII art, because without it the resulting resolution would only have been 80×100.
Composite video display.
For this reason, using an RGBI color monitor was the preferred configuration.
A flaw turned into an advantage.
However, programmers soon found out that this flaw could be turned into an asset, as distinct patterns of high-resolution dots would "smear" into consistent areas of solid colors, thus allowing the display of completely new colors. Since these new colors are the result of cross-color artifacting, they are often called "artifact colors". Both the standard 320×200 four-color and the 640×200 black-and-white graphics modes could be used with this technique.
Internal operation.
"Direct" colors, that is, the normal colors that are seen on an RGB monitor, are encoded by the CGA's onboard hardware into an NTSC-compatible signal fed to the card's RCA output jack. For cost reasons, this is not done using an RGB-to-YIQ converter as called for by the NTSC standard, but by a series of flip-flops and delay lines. Consequently, these colors look somewhat different on a composite color monitor compared to RGB; most notably, color 6 again looks dark-yellow instead of brown.
As stated before, "artifact" colors are seen because the composite monitor's NTSC chroma decoder misinterprets some of the luminance information as color. By carefully placing pixels in appropriate patterns, the skilled programmer produces particular cross-color artifacts yielding the desired color; either from purely black-and-white pixels in 640×200 mode, or resulting from a "combination" of "direct" and "artifact" colors in 320×200 mode, as seen in these pictures.
Thus, with the choice of 320×200 vs. 640×200 mode, the choice of palette (1 or 2) and the freely-selectable color 0 in 320×200 modes (see above), each one of these parameters results in a different set of artifact colors, making for a total gamut of well over a hundred colors, of which 16 can be displayed at the same time.
Availability and caveats.
The 320×200 variant of this technique (see above) was is how the standard BIOS-supported graphics mode looks on a composite color monitor. The 640×200 variant however requires modifying a bit (color burst disable) directly in the CGA's hardware registers, as a result, it is usually referred to as a separate "mode", often just as "the" composite color mode, since its more distinctive set of artifact colors led it to being more commonly used than the 320×200 variant.
Being completely dependent on the NTSC encoding/decoding process, composite color artifacting is not available on an RGBI monitor, nor is it emulated by EGA, VGA or contemporary graphics adapters.
Using the same monitor at the same settings, "direct" colors are constant from card to card and host system to host system. "Artifact" colors, on the other hand, tend to drift in hue. (This is unrelated to the hue shift problem encountered in the terrestrial reception of NTSC broadcast signals.) For this reason, the original IBM PC and XT design provides a trimpot labeled "COLOR ADJUST" which modifies the phase difference between the ISA bus' CLK and OSC signals that leaves "direct" colors constant while changing the hue of "artifact" colors.
Host systems that lack a "COLOR ADJUST" trimpot, such as the Tandy 1000's internal video hardware, might produce erratic artifact colors and require hue adjustment on the composite color monitor. Later systems do not provide a proper OSC signal at all, rendering the composite color display completely unusable.
Resolution and usage.
Due to the relationship between the CGA's pixel clock and the NTSC color subcarrier, the effective horizontal resolution is reduced to 160 pixels of any color, or 320 pixels when limiting oneself to black and white pixels.
Bugs and errata.
In 80-column text mode, bad prioritization of granting access to the single-ported display RAM gives the host system's CPU a higher priority than the video hardware, resulting in display errors ("snow") whenever the CPU accesses display RAM. Prudent programmers compensated by limiting display RAM accesses to the retrace period. This problem is neither present in any other display mode, nor in most other display adapters.
The video controller 6845's row counter being only seven bits wide, display RAM in graphics modes is laid out in a 2:1 "interlace" pattern, first laying out the data for rows 0, 2, 4, etc., then the data for rows 1, 3, 5, etc., adding additional software overhead for display RAM manipulation. This is unrelated to the NTSC field interlace.
As previously mentioned, IBM designed the 5153 CGA monitor to intentionally darken color index #6 from dark yellow to brown; however, some clone monitors did not have this circuitry. On such monitors, or 5153 monitors where this circuitry had failed, color index #6 will remain dark yellow (see color example).
The total amount of video memory on a CGA card (16384 total bytes) is not fully utilised by all BIOS-initiated video modes (40×25 and 80×25 text modes, 320×200 and 640×200 graphics modes). Only by setting up video modes manually using CGA port writes can all 16384 bytes be displayed as pixel elements simultaneously.
Connector.
The Color Graphics Adapter uses a standard DE-9 connector.
Competing adapters.
A less widely-used competitor was the Plantronics Colorplus, a CGA-compatible card which doubles the video RAM to 32k, thus allowing 16 colors at 320×200 resolution and 4 colors at 640×200 resolution. The "extended CGA" modes provided by the IBM PCjr and Tandy 1000 are similar to these modes.
Another extension in some CGA-compatible chipsets (including those in the Olivetti M24, the DEC VAXmate, and some Compaq and Toshiba portables) is a doubled vertical resolution. This gives a higher-quality text display and an extra 640×400 graphics mode.
The CGA card was succeeded in the consumer space by IBM's Enhanced Graphics Adapter (EGA) card, which supports most of CGA's modes, and added an additional resolution (640×350) as well as a software-selectable palette of 16 colors out of 64 in both text and graphics modes.
Market penetration.
When IBM introduced its PC in 1981, the CGA standard, though introduced at the same time, was used relatively little at first. Most people bought PCs for business computing. For gaming, other brands of home computers were much more popular, and at that time color graphics were considered to have little more than toy value. Thus, most early PC buyers opted for the cheaper text-only Monochrome Display Adapter (MDA) instead of CGA.
In 1982 came the introduction of the Hercules Graphics Card, which offered monochrome-only graphics at a much higher resolution than the CGA card and was more compatible with MDA, further eroding CGA's market share. The HGC was arguably the most commonly-utilized card connected to monochrome monitors throughout the IBM PC's life.
Things changed in 1984 when IBM introduced the PC AT and the Enhanced Graphics Adapter (EGA). Along with this move, the price of the older CGA card was lowered considerably; it now became an attractive low-cost option and was soon adopted by the new PC cloning companies as well. Entry-level non-AT PCs with CGA graphics sold very well during the next few years, and consequently there were many games released for such systems, despite their limitations. CGA's popularity started to wane after VGA became IBM's high-level standard and EGA the entry-level standard in 1987.