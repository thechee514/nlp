ASCII.
American Standard Code for Information Interchange (ASCII), "ASK-ee," is a character encoding based on the English alphabet. ASCII codes represent text in computers, communications equipment, and other devices that work with text. Most modern character encodings — which support many more characters than did the original — have a historical basis in ASCII.
Work on ASCII began in 1960. The first edition of the standard was published in 1963, a major revision in 1967, and the most recent update in 1986. It currently defines codes for 128 characters: 33 are non-printing, mostly obsolete control characters that affect how text is processed, and 94 are printable characters (excluding the space).
I have also approved recommendations of the Secretary of Commerce regarding standards for recording the Standard Code for Information Interchange on magnetic tapes and paper tapes when they are used in computer operations.
All computers and related equipment configurations brought into the Federal Government inventory on and after July 1, 1969, must have the capability to use the Standard Code for Information Interchange and the formats prescribed by the magnetic tape and paper tape standards when these media are used.
Overview.
Like other character encodings, ASCII specifies a correspondence between digital bit patterns and character symbols (i.e. graphemes and control characters). This allows digital devices to communicate with each other and to process, store, and communicate character-oriented information such as written language. The ASCII character encoding — or a compatible extension (see below) — is used on nearly all common computers, especially personal computers and workstations. The preferred MIME name for this encoding is "US-ASCII".
Except for a few of the ASCII control characters that prescribe some elementary line-oriented formatting, ASCII does not define any mechanism for describing the structure or appearance of text within a document. Other schemes, such as markup languages, address page and document layout and formatting.
History.
ASCII is, strictly, a seven-bit code, meaning it uses patterns of seven binary digits (a range of 0 to 127 decimal) to represent each character. When ASCII was introduced, many computers used eight-bit bytes (groups of bits), also called octets, as the native data type. In seven-bit ASCII encoding, the eighth bit was commonly used as a parity bit for error checking on communication lines or for other device-specific functions. Machines that did not use parity checking typically set the eighth bit to "0".
The American National Standards Institute (then called the American Standards Association or ASA, and later the United States of America Standards Institute or USASI) developed ASCII based on earlier teleprinter encoding systems. Circa 1956, Ivan Idelson, at Ferranti in the UK, had proposed the Cluff-Foster-Idelson coding of characters on 7 track paper tape to a British Standards committee. This was one of the influences on ASCII. ASCII itself first entered commercial use in 1963 as a seven-bit teleprinter code for American Telephone & Telegraph's TWX (Teletype Wide-area eXchange) network. TWX originally used the earlier five-bit Baudot code, which was also used by the competing Telex teleprinter system. The Bell System had planned to upgrade to a six-bit code derived from the Fieldata project, which contained many punctuation and control characters that the Baudot code did not, but was persuaded instead to join the American Standards Association (part of ANSI) subcommittee that had started to develop ASCII. Compared with earlier telegraph codes, the proposed Bell code and ASCII both underwent re-ordering for more convenient sorting (especially alphabetization) of lists, and added features for devices other than teleprinters. Bob Bemer introduced features such as the escape sequence. His British colleague Hugh McGregor Ross helped to popularize this work — according to Bemer, "so much so that the code that was to become ASCII was first called the Bemer-Ross Code in Europe".
ASCII was published as ASA X3.4-1963 and then subsequently updated as USASI X3.4-1967, USASI X3.4-1968, ANSI X3.4-1977, and finally, ANSI X3.4-1986.
Other international standards bodies have ratified character encodings such as IEC 646 that are identical or nearly identical to ASCII, with extensions for characters outside the English alphabet and symbols used outside the United States, such as the symbol for the United Kingdom's pound sterling (£). Almost every country needed an adapted version of ASCII since ASCII only suited the needs of the USA and a few other countries. For example, Canada had its own version that supported French. Although these encodings are sometimes referred to as ASCII, true ASCII is strictly defined only by ANSI standard.
ASCII has been incorporated into the Unicode character set as the first 128 symbols, so the ASCII characters have the same numeric codes in both sets. This allows UTF-8 to be backward compatible with ASCII, a significant advantage.
ASCII control characters.
ASCII reserves the first 32 codes (numbers 0–31 decimal) for control characters: codes originally intended not to carry printable information, but rather to control devices (such as printers) that make use of ASCII, or to provide meta-information about data streams such as those stored on magnetic tape. For example, character 10 represents the "line feed" function (which causes a printer to advance its paper), and character 8 represents "backspace".
The original ASCII standard used only short descriptive phrases for each control character. The ambiguity this left was sometimes intentional (where a character would be used slightly differently on a terminal link than on a data stream) and sometimes more accidental (such as what "delete" means).
Probably the most influential single device on the interpretation of these characters was the ASR-33 Teletype series, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage up through the 1980s, lower cost and in some ways less fragile than magnetic tape. In particular, the Teletype 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (DELete) became de-facto standards. Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as "delete previous character" was also adopted by many early timesharing systems but eventually faded out.
The use of Control-S (XOFF, an abbreviation for "transmit off") as a handshaking signal warning a sender to stop transmission because of impending overflow, and Control-Q (XON, "transmit on") to resume sending, persists to this day in many systems as a manual output control technique. On some systems Control-S retains its meaning but Control-Q is replaced by a second Control-S to resume output.
Code 127 is officially named "delete" but the Teletype label was "rubout". Since the original standard gave no detailed interpretation for most control codes, interpretations of this code varied. The original Teletype meaning, and the intent of the standard, was to make it an ignored character, the same as NUL (all zeroes). This was specifically useful for paper tape, because punching the all-ones bit pattern on top of an existing mark would obliterate it. Tapes designed to be "hand edited" could even be produced with spaces of extra NULs (blank tape) so that a block of characters could be "rubbed out" and then replacements put into the empty space.
As video terminals began to replace printing ones, the value of the "rubout" character was lost. DEC systems, for example, interpreted "Delete" to mean "remove the character before the cursor," and this interpretation also became common in Unix systems. Most other systems used "Backspace" for that meaning and used "Delete" as it was used on paper tape, to mean "remove the character after the cursor". That latter interpretation is the most common today.
Many more of the control codes have taken on meanings quite different from their original ones. The "escape" character (code 27), for example, was originally intended to allow sending other control characters as literals instead of invoking their meaning. This is the same meaning of "escape" encountered in URL encodings, C language strings, and other systems where certain characters have a reserved meaning. Over time this meaning has been coopted and has eventually drifted. In modern use, an ESC sent to the terminal usually indicates the start of a command sequence, usually in the form of an ANSI escape code. An ESC sent from the terminal is most often used as an "out of band" character used to terminate an operation, as in the TECO and vi text editors.
The inherent ambiguity of many control characters, combined with their historical usage, has also created problems when transferring "plain text" files between systems. The clearest example of this is the newline problem on various operating systems. On printing terminals there is no question that you terminate a line of text with both "Carriage Return" and "Linefeed". The first returns the printing carriage to the beginning of the line and the second advances to the next line without moving the carriage. However, requiring two characters to mark the end of a line introduced unnecessary complexity and questions as to how to interpret each character when encountered alone. To simplify matters, plain text files on Unix systems use line feeds alone to separate lines. Similarly, older Macintosh systems, among others, use only carriage returns in plain text files. Various DEC operating systems used both characters to mark the end of a line, perhaps for compatibility with teletypes, and this de facto standard was copied in the M operating system and then in MS-DOS and eventually Microsoft Windows. Transmission of text over the Internet, for protocols as E-mail and the World Wide Web, uses both characters. The DEC operating systems, along with CP/M, tracked file length only in units of disk blocks and used Control-Z (SUB) to mark the end of the actual text in the file (also done for CP/M compatibility in some cases in MS-DOS, though MS-DOS has always recorded exact file-lengths). Control-C (ETX, End of TeXt) might have made more sense, but was already in wide use as a program abort signal. UNIX's use of Control-D (EOT, End of Transmission) appears on its face similar, but is used only from the terminal and never stored in a file.
While the codes mentioned above have retained some semblance of their original meanings, many of the codes originally intended for stream delimiters or for link control on a terminal have lost all meaning except their relation to a letter. Control-A is almost never used to mean "start of header" except on an ANSI magnetic tape. When connecting a terminal to a system, or asking the system to recognize that a logged-out terminal wants to log in, modern systems are much more likely to want a carriage return or an ESCape than Control-E (ENQuire, meaning "is there anybody out there?").
The abbreviation ASCIIZ or ASCIZ refers to a null-terminated ASCII string (also known as a C string).
Non-whitespace control characters.
refers to NO-WS-CTL, non-whitespace control characters. These are control characters that do not include carriage return, line feed, and white space characters (see), i.e.: decimal 1–8, 11–12, 14–31, and 127.
ASCII printable characters.
Code 32, the "space" character, denotes the space between words, as produced by the space-bar of a keyboard. Codes 33 to 126, known as the printable characters, represent letters, digits, punctuation marks, and a few miscellaneous symbols.
Seven-bit ASCII provided seven "national" characters and, if the combined hardware and software permit, can use overstrikes to simulate some additional international characters: in such a scenario a backspace can precede a grave accent (which the American and British standards, but only those standards, also call "opening single quotation mark"), a backtick, or a breath mark (inverted vel).
Aliases.
Of these, only the aliases "US-ASCII" and "ASCII" have achieved widespread use. One often finds them in the optional "charset" parameter in the Content-Type header of some MIME messages, in the equivalent "meta" element of some HTML documents, and in the encoding declaration part of the prolog of some XML documents.
Variants.
As computer technology spread throughout the world, different standards bodies and corporations developed many variations of ASCII in order to facilitate the expression of non-English languages that used Roman-based alphabets. One could class some of these variations as "ASCII extensions", although some misuse that term to cover all variants, including those that do not preserve ASCII's character-map in the 7-bit range.
The PETSCII Code used by Commodore International for their 8-bit systems is probably unique among post-1970 codes in being based on ASCII-1963 instead of the far more common ASCII-1967.
Incompatibility vs interoperability.
IEC 646 (1972), the first attempt to remedy ASCII's English language bias, created compatibility problems, since it remained a 7-bit character-set. It made no additional codes available, so it reassigned some in language-specific variants. Escape codes were defined to indicate which national variant applied to a piece of text, but these were rarely used, so it was often impossible to know what variant to work with and therefore which character a code represented, and text-processing systems could generally cope with only one variant anyway.
A German, French, or Swedish, etc., programmer had to get used to
codice_1
codice_2
Eventually, improved technology brought out-of-band means to represent the information formerly encoded in the eighth bit of each byte, freeing this bit to add another 128 additional character-codes for new assignments.
For example, IBM developed 8-bit code pages, such as code page 437, which replaced the control-characters with graphic symbols such as smiley faces, and mapped additional graphic characters to the upper 128 positions. Operating systems such as DOS supported these code-pages, and manufacturers of IBM PCs supported them in hardware. Digital Equipment Corporation developed the Multinational Character Set (DEC-MCS) for use in the popular VT220 terminal.
Eight-bit standards such as IEC 8859 (derived from the DEC-MCS) and Mac OS Roman developed as true extensions of ASCII, leaving the original character-mapping intact, but adding additional character definitions after the first 128 (i.e., 7-bit) characters. This enabled representation of characters used in a broader range of languages. But these standards continued to suffer from incompatibilities and limitations. Still, ISO-8859-1 (Latin 1), its variant Windows-1252 (often mislabeled as ISO-8859-1), and the original 7-bit ASCII remain the most common character encodings in use today.
Unicode.
Unicode and the ISO/IEC 10646 Universal Character Set (UCS) have a much wider array of characters, and their various encoding forms have begun to supplant ISO/IEC 8859 and ASCII rapidly in many environments. While ASCII is limited to 128 characters, Unicode and the UCS support more characters by separating the concepts of unique identification (using natural numbers called "code points") and encoding (to 8-, 16- or 32-bit binary formats, called UTF-8, UTF-16 and UTF-32).
To permit backward compatibility, the 128 ASCII and 256 ISO-8859-1 (Latin 1) characters are assigned Unicode/UCS code points that are the same as their codes in the earlier standards. Therefore, ASCII can be considered a 7-bit encoding scheme for a very small subset of Unicode/UCS, and, conversely, the UTF-8 encoding forms are binary-compatible with ASCII for code points below 128, meaning every properly encoded ASCII file is also a valid UTF-8 file. The other encoding forms resemble ASCII in how they represent the first 128 characters of Unicode, but use 16 or 32 bits per character, so they require conversion for compatibility.
Order.
The slang expression "ASCIIbetical" is sometimes used for this order.
In programming, "alphanumeric" sorting means to sort by numeric value, without regard of any character set. An alphanumerically sorted array of bytes will appear ASCIIbetically when viewed in an ASCII-compatible character set.
A refined version of this order converts uppercase letters to lowercase before comparing ASCII values.